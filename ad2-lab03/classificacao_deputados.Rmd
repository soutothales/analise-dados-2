---
title: "R Notebook"
output: html_notebook
---


```{r}
library(rpart)
library(caret)
library(dplyr)
```

```{r}
eleicoes = read.csv("train.csv", encoding="utf-8")
```

##1. Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador?

Para responder esta questão iremos plotar a diferença entre as duas classes: eleito e não eleito.

```{r}
df_classes <- eleicoes %>% count(situacao_final)

df_classes
```


Para ilustrar melhor, plotamos o gráfico com a proporção:

```{r}
proporcao <- (df_classes$n/sum(df_classes$n) * 100)
ggplot(df_classes, aes(x = situacao_final, y = proporcao), options(scipen=5)) +
    geom_bar(stat = "identity")
```

Foi percebido que a classe nao_eleito (não eleito) possui muito mais instâncias que a classe eleito. eleito possui 416 instâncias e nao_eleito possui 3719 instâncias, com proporção de 10,1% e 89,9%. Ao treinar um modelo futuro ele tende a ficar enviesado visto que a diferença entre nao_eleito e eleito é visivelmente grande.
\  
\  
\  

##2. Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo. 

Podemos escolher algumas variáveis com as quais queremos trabalhar:

```{r}
filtra_dados <- eleicoes %>% select(total_despesa, descricao_cor_raca, estado_civil, grau, situacao_final)
```


```{r}
# Separa os dados em treino e teste
dataPartition <- createDataPartition(y = filtra_dados$situacao_final , p=0.70, list=FALSE)

treino <- filtra_dados[ dataPartition, ]
teste <- filtra_dados[ -dataPartition, ]
```
\  
\  

Já que vimos na questão anterior que as classes estão desbalanceadas, é importante balanceá-las para depois treinar o modelo. Escolhemos usar a ideia de under-sampling.

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     #number = 10, 
                     repeats = 4,
                     verboseIter = FALSE,
                     sampling = "down")


formula = as.formula(situacao_final ~ .)
```
\  
\  
\  

### Regressão logística:
```{r warning = FALSE}
reg_log1 <- train(formula,
                  data = treino,
                  method="glm",
                  trControl = ctrl, 
                  family="binomial",      # se a variável for binária
                  na.action = na.omit)

reg_log1
```
\  
\  
\  

### Árvore de decisão:
```{r}
arvore <- train(formula,
                 data = treino,
                 method = "rpart",
                 trControl = ctrl,
                 minsplit=30,
                 cp=0.001,  # parâmetro de complexidade
                 maxdepth=30)

arvore
```
\  
\  
\  

### Adaboost:
```{r}
boost <- train(formula,
                data=treino,
                trControl = ctrl,
                method = "adaboost")

boost
```
\  
\  
\  

##3. Reporte acurácia, precision, recall e f-measure no treino e validação. Como você avalia os resultados? Justifique sua resposta.

Acurácia = (TP + TN)/(TP + TN + FP + FN) 
<i>Nos diz a proporção de observações corretamente classificadas.</i>

Precision =  TP / (TP + FP)
<i> Diz respeito a quantas das observaçoes preditas como positivas são realmente positivas </i>

Recall = TP / (TP + FN)
<i>  Diz respeito a quantas das observaçoes positivas foram corretamente classificadas </i>

F-Measure = 2 x (Recall x Precision) / (Recall + Precision)


### Regressão logística no treino:
```{r}
treino$predicao_log <- predict(reg_log1, treino)

TP <- treino %>% filter(situacao_final == "eleito", predicao_log == "eleito") %>% nrow()
TN <- treino %>% filter(situacao_final == "nao_eleito" , predicao_log == "nao_eleito" ) %>% nrow()
FP <- treino %>% filter(situacao_final == "nao_eleito" , predicao_log == "eleito") %>% nrow() 
FN <- treino %>% filter(situacao_final == "eleito", predicao_log == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)


accuracy
precision
recall
f_measure

```

### Regressão logística no teste:
```{r}
teste$predicao_log <- predict(reg_log1, teste)

TP <- teste %>% filter(situacao_final == "eleito", predicao_log == "eleito") %>% nrow()
TN <- teste %>% filter(situacao_final == "nao_eleito" , predicao_log == "nao_eleito" ) %>% nrow()
FP <- teste %>% filter(situacao_final == "nao_eleito" , predicao_log == "eleito") %>% nrow() 
FN <- teste %>% filter(situacao_final == "eleito", predicao_log == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure

```

Os valores de acurácia, precisão, recall e f-measure são maiores na partição de teste do que na partição de treino.



### Árvore de decisão no treino:
```{r}
treino$predicao_arvore <- predict(arvore, treino)

TP <- treino %>% filter(situacao_final == "eleito", predicao_arvore == "eleito") %>% nrow()
TN <- treino %>% filter(situacao_final == "nao_eleito" , predicao_arvore == "nao_eleito" ) %>% nrow()
FP <- treino %>% filter(situacao_final == "nao_eleito" , predicao_arvore == "eleito") %>% nrow() 
FN <- treino %>% filter(situacao_final == "eleito", predicao_arvore == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```


### Árvore de decisão no teste:
```{r}
teste$predicao_arvore <- predict(arvore, teste)

TP <- teste %>% filter(situacao_final == "eleito", predicao_arvore == "eleito") %>% nrow()
TN <- teste %>% filter(situacao_final == "nao_eleito" , predicao_arvore == "nao_eleito" ) %>% nrow()
FP <- teste %>% filter(situacao_final == "nao_eleito" , predicao_arvore == "eleito") %>% nrow() 
FN <- teste %>% filter(situacao_final == "eleito", predicao_arvore == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```
Ao contrário do modelo de Regressão Logística, o modelo de Árvore de Decisão apresenta valores maiores na partição de treino.



### Adaboost no treino:
```{r}
treino$predicao_ada <- predict(boost, treino)

TP <- treino %>% filter(situacao_final == "eleito", predicao_ada == "eleito") %>% nrow()
TN <- treino %>% filter(situacao_final == "nao_eleito" , predicao_ada == "nao_eleito" ) %>% nrow()
FP <- treino %>% filter(situacao_final == "nao_eleito" , predicao_ada == "eleito") %>% nrow() 
FN <- treino %>% filter(situacao_final == "eleito", predicao_ada == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```

### Adaboost no teste:
```{r}
teste$predicao_ada <- predict(boost, teste)

TP <- teste %>% filter(situacao_final == "eleito", predicao_ada == "eleito") %>% nrow()
TN <- teste %>% filter(situacao_final == "nao_eleito" , predicao_ada == "nao_eleito" ) %>% nrow()
FP <- teste %>% filter(situacao_final == "nao_eleito" , predicao_ada == "eleito") %>% nrow() 
FN <- teste %>% filter(situacao_final == "eleito", predicao_ada == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```
Como podemos notar, o modelo na partição treino apresenta acurácia, precisão, recall e f-measure maior que no teste, igualmente ao modelo de Árvore de Decisão.


```{r}
confusionMatrix(teste$predicao_log, teste$situacao_final)
confusionMatrix(teste$predicao_arvore, teste$situacao_final)
confusionMatrix(teste$predicao_ada, teste$situacao_final)
```


##4. Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo? Crie pelo menos um novo atributo que não está nos dados originais e estude o impacto desse atributo

```{r}

```


##5. Envie seus melhores modelos à competição do Kaggle. Sugestões abaixo:
###1. Experimente outros modelos (e.g. SVM, RandomForests e GradientBoosting)
###2. Crie novos atributos.


```{r}
kaggle_teste <- read.csv("test.csv", encoding="UTF-8")
```

```{r warning = FALSE}
ctrl2 <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10,
                     verboseIter = FALSE,
                     sampling = "down")

kaggle_reg_log <- train(formula,
                        data = filtra_dados,
                        method="glm",
                        trControl = ctrl2, 
                        family="binomial",      # se a variável for binária
                        na.action = na.omit)

summary(kaggle_reg_log)
```


```{r}


predicao_kaggle <- predict(kaggle_reg_log, kaggle_teste)

kaggle_teste$situacao_final <- predicao_kaggle

kaggle_teste <- kaggle_teste %>% select(ID, situacao_final)

```


```{r}
write.csv(kaggle_teste, file = "kaggle_teste.csv", row.names = FALSE)
```




