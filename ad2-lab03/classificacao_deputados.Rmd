---
title: "R Notebook"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(rpart)
library(caret)
library(dplyr)
```

```{r}
eleicoes = read.csv("train.csv", encoding="utf-8")
```

##1. Há desbalanceamento das classes (isto é, uma classe tem muito mais instâncias que outra)? Em que proporção? Quais efeitos colaterais o desbalanceamento de classes pode causar no classificador?

Para responder esta questão iremos plotar a diferença entre as duas classes: eleito e não eleito.

```{r}
df_classes <- eleicoes %>% count(situacao_final)

df_classes
```
Para ilustrar melhor, plotamos o gráfico com a proporção:

```{r}
proporcao <- (df_classes$n/sum(df_classes$n) * 100)
ggplot(df_classes, aes(x = situacao_final, y = proporcao), options(scipen=5)) +
    geom_bar(stat = "identity")
```

Foi percebido que a classe nao_eleito (não eleito) possui muito mais instâncias que a classe eleito. eleito possui 416 instâncias e nao_eleito possui 3719 instâncias, com proporção de 10,1% e 89,9%. Ao treinar um modelo futuro ele tende a ficar enviesado visto que a diferença entre nao_eleito e eleito é visivelmente grande.
\  
\  
\  

##2. Treine: um modelo de regressão logística, uma árvore de decisão e um modelo de adaboost. Tune esses modelos usando validação cruzada e controle overfitting se necessário, considerando as particularidades de cada modelo. 

```{r}
# Separa os dados em treino e teste
dataPartition <- createDataPartition(y = eleicoes$situacao_final , p=0.70, list=FALSE)

treino <- eleicoes[ dataPartition, ]
teste <- eleicoes[ -dataPartition, ]
```
\  
\  

Já que vimos na questão anterior que as classes estão desbalanceadas, é importante balanceá-las para depois treinar o modelo. Escolhemos usar a ideia de under-sampling.

```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "down")


formula = as.formula(situacao_final ~ total_despesa + descricao_cor_raca + estado_civil + grau)
```
\  
\  
\  

### Regressão logística:
```{r warning = FALSE}
reg_log1 <- train(formula,
                 data = treino,
                 method="glm",
                 trControl = ctrl, 
                 family="binomial",      # se a variável for binária
                 na.action = na.omit)

reg_log1
```
\  
\  
\  

### Árvore de decisão:
```{r}
arvore <- train(formula,
                 data = treino,
                 method = "rpart",
                 trControl = ctrl,
                 minsplit=30,
                 cp=0.001,  # parâmetro de complexidade
                 maxdepth=30)

arvore
```
\  
\  
\  

### Adaboost:
```{r}
boost <- train(formula,
                data=treino,
                trControl = ctrl,
                method = "adaboost")

boost
```
\  
\  
\  

##3. Reporte acurácia, precision, recall e f-measure no treino e validação. Como você avalia os resultados? Justifique sua resposta.

Acurácia = (TP + TN)/(TP + TN + FP + FN) 
<i>Nos diz a proporção de observações corretamente classificadas.</i>

Precision =  TP / (TP + FP)
<i> Diz respeito a quantas das observaçoes preditas como positivas são realmente positivas </i>

Recall = TP / (TP + FN)
<i>  Diz respeito a quantas das observaçoes positivas foram corretamente classificadas <i>

F-Measure = 2 x (Recall x Precision) / (Recall + Precision)


### Regressão logística:
```{r}
teste$predicao_log <- predict(reg_log1, teste)

TP <- teste %>% filter(situacao_final == "eleito", predicao_log == "eleito") %>% nrow()
TN <- teste %>% filter(situacao_final == "nao_eleito" , predicao_log == "nao_eleito" ) %>% nrow()
FP <- teste %>% filter(situacao_final == "nao_eleito" , predicao_log == "eleito") %>% nrow() 
FN <- teste %>% filter(situacao_final == "eleito", predicao_log == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```



### Árvore de decisão:
```{r}
teste$predicao_arvore <- predict(arvore, teste)

TP <- teste %>% filter(situacao_final == "eleito", predicao_arvore == "eleito") %>% nrow()
TN <- teste %>% filter(situacao_final == "nao_eleito" , predicao_arvore == "nao_eleito" ) %>% nrow()
FP <- teste %>% filter(situacao_final == "nao_eleito" , predicao_arvore == "eleito") %>% nrow() 
FN <- teste %>% filter(situacao_final == "eleito", predicao_arvore == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```



### Adaboost:
```{r}
teste$predicao_ada <- predict(boost, teste)

TP <- teste %>% filter(situacao_final == "eleito", predicao_ada == "eleito") %>% nrow()
TN <- teste %>% filter(situacao_final == "nao_eleito" , predicao_ada == "nao_eleito" ) %>% nrow()
FP <- teste %>% filter(situacao_final == "nao_eleito" , predicao_ada == "eleito") %>% nrow() 
FN <- teste %>% filter(situacao_final == "eleito", predicao_ada == "nao_eleito" ) %>% nrow()

accuracy <- (TP + TN)/(TP + TN + FP + FN) 
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f_measure <- 2*(recall * precision) / (recall + precision)

accuracy
precision
recall
f_measure
```


Para uma análise mais otimista podemos levar em consideração o Recall

```{r}
confusionMatrix(teste$predicao_log, teste$situacao_final)
```


##4. Interprete as saídas dos modelos. Quais atributos parecem ser mais importantes de acordo com cada modelo? Crie pelo menos um novo atributo que não está nos dados originais e estude o impacto desse atributo

```{r}
varImp(reg_log1)
varImp(arvore)
varImp(boost)
```


##5. Envie seus melhores modelos à competição do Kaggle. Sugestões abaixo:
###1. Experimente outros modelos (e.g. SVM, RandomForests e GradientBoosting)
###2. Crie novos atributos.




